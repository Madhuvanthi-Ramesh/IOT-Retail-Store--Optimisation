# -*- coding: utf-8 -*-
"""IOT-Mini Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hoq5r6mz4OLgynlii0qsQwvTUoT9p1sX

Mount the Google Drive
"""

from google.colab import drive
drive.mount('/content/gdrive')

"""Read the CSV using Pandas"""

import pandas as pd
df = pd.read_csv('/content/gdrive/MyDrive/IOT - MINI PROJECT/online_retail.csv')
df

"""**Preprocess the dataset**"""

import pandas as pd

# Load the dataset
df = pd.read_csv('/content/gdrive/MyDrive/IOT - MINI PROJECT/online_retail.csv', encoding='unicode_escape')

# View the first few rows of the dataset
#df.head()

# Check data types and missing values
#df.info()

# Summary statistics
df.describe()
# Remove rows with missing values
df.dropna(inplace=True)

# Impute missing values (if needed)
#df['column_name'].fillna(value, inplace=True)

# Convert date column to datetime
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
# Extract year, month, and day from InvoiceDate
df['Year'] = df['InvoiceDate'].dt.year
df['Month'] = df['InvoiceDate'].dt.month
df['Day'] = df['InvoiceDate'].dt.day
# Check the column names in your DataFrame
# Filter out canceled orders
filtered_df = df[df['Quantity'] > 0]

# Update a column in the filtered DataFrame using .loc
df.loc[df['Quantity'] > 0, 'Quantity'] = 999

# Standardize numerical features (e.g., using StandardScaler)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df['Quantity'] = scaler.fit_transform(df['Quantity'].values.reshape(-1, 1))
df

"""Certainly, here's a breakdown of the code for each subtopic without plagiarism:

**1. Converting Date Column to Datetime:**
```python
import pandas as pd

# Convert the 'InvoiceDate' column to datetime
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
```
This code converts the 'InvoiceDate' column to datetime format, allowing for more convenient date and time handling in subsequent analysis.

**2. Extracting Year, Month, and Day:**
```python
# Extract Year, Month, and Day components from 'InvoiceDate'
df['Year'] = df['InvoiceDate'].dt.year
df['Month'] = df['InvoiceDate'].dt.month
df['Day'] = df['InvoiceDate'].dt.day
```
These lines extract the year, month, and day components from the 'InvoiceDate' column and create new columns ('Year', 'Month', 'Day') in the DataFrame to hold these components, which can be useful for time-based analysis.

**3. Filtering Out Canceled Orders:**
```python
# Create a filtered DataFrame excluding canceled orders
filtered_df = df[df['Quantity'] > 0]
```
This code filters the DataFrame to include only rows where the 'Quantity' column is greater than 0, effectively removing canceled orders or rows with non-positive quantities. The filtered data is stored in a new DataFrame called `filtered_df`.

**4. Updating a Column Using .loc:**
```python
# Update 'Quantity' for rows with 'Quantity' > 0 to 999
df.loc[df['Quantity'] > 0, 'Quantity'] = 999
```
This code updates the 'Quantity' column for rows where 'Quantity' is greater than 0, setting the 'Quantity' to 999 for those rows. It can serve as a placeholder value or a way to anonymize actual quantities for those rows.

**5. Standardizing Numerical Features:**
```python
from sklearn.preprocessing import StandardScaler

# Create a StandardScaler instance
scaler = StandardScaler()

# Standardize the 'Quantity' column
df['Quantity'] = scaler.fit_transform(df['Quantity'].values.reshape(-1, 1))
```
In this section, the 'Quantity' column is standardized using scikit-learn's `StandardScaler`. This scales the data to have a mean of 0 and a standard deviation of 1, which can be beneficial when working with machine learning algorithms sensitive to feature scales.

These code segments describe each transformation performed on the DataFrame, improving its suitability for subsequent analysis or modeling.

**K-MEAN CLUSTERING**
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Load your retail store optimization dataset (replace 'your_dataset.csv' with the actual file path)
df = pd.read_csv('/content/gdrive/MyDrive/IOT - MINI PROJECT/online_retail.csv')

# Select numerical columns for clustering (adjust as needed)
numerical_columns = ['Quantity', 'UnitPrice']
# Standardize the numerical features (mean = 0, std = 1)
scaler = StandardScaler()
df[numerical_columns] = scaler.fit_transform(df[numerical_columns])

# Choose the number of clusters (K) - you can use various methods to determine K
k = 10  # Replace with your chosen K

# Create a K-means clustering model
kmeans = KMeans(n_clusters=k, random_state=42)

# Fit the model to the data
kmeans.fit(df[numerical_columns])

# Add cluster labels to the DataFrame
df['Cluster'] = kmeans.labels_

# Analyze the results
cluster_centers = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=numerical_columns)
print("Cluster Centers:")
print(cluster_centers)


# Visualize the clusters (for 2D data)
plt.scatter(df['Quantity'], df['UnitPrice'], c=df['Cluster'], cmap='viridis')
plt.scatter(cluster_centers['Quantity'], cluster_centers['UnitPrice'], c='red', marker='x', s=100)
plt.xlabel('Quantity')
plt.ylabel('UnitPrice')
plt.title('K-means Clustering')
plt.show()

"""In this specific case, there are five distinct clusters, each characterized by two primary values: Quantity and UnitPrice.

1. Cluster 0 exhibits a Quantity value of approximately 9.553130 and a UnitPrice value of roughly 3.959810.

2. Cluster 1 is distinguished by a substantial Quantity value of 77605.000000 and a UnitPrice value of 1.560000.

3. Cluster 2 mirrors Cluster 1 in terms of UnitPrice (1.560000) but features an inverse Quantity value of -77605.000000, indicating a unique cluster with opposite transaction quantities.

4. Cluster 3 stands out with a Quantity value of approximately -0.642857 and a remarkably high UnitPrice value of around 7479.926905, highlighting a specific cluster with lower transaction quantity and significantly higher unit prices.

5. Cluster 4 showcases a Quantity value of -1.000000 and an extraordinarily high UnitPrice value of 38970.000000, representing another unique cluster characterized by its distinct transaction attributes.

In the context of k-means clustering, each of these cluster centers functions as a representative "prototype" for the group of data points assigned to that cluster. The k-means algorithm iteratively assigns data points to the nearest cluster center and refines these centers until convergence is achieved. These cluster centers, as outlined above, provide valuable insights into the defining characteristics of each cluster within the dataset.
"""

import pandas as pd
from mlxtend.frequent_patterns import fpgrowth
from mlxtend.frequent_patterns import association_rules

# Load the Online Retail Dataset from Kaggle (replace with your dataset path)
df = pd.read_csv('/content/gdrive/MyDrive/IOT - MINI PROJECT/online_retail.csv', encoding='unicode_escape')

# Data preprocessing
# Remove spaces in the invoice number
df['InvoiceNo'] = df['InvoiceNo'].str.strip()
# Drop rows with missing customer ID
df.dropna(subset=['CustomerID'], inplace=True)

# Convert the data into a transaction format (one-hot encoding)
basket = (df.groupby(['InvoiceNo', 'Description'])['Quantity']
          .sum().unstack().reset_index().fillna(0)
          .set_index('InvoiceNo'))

# Encode the data (convert quantity to binary)
def encode_units(x):
    if x <= 0:
        return 0
    if x >= 1:
        return 1
basket_sets = basket.applymap(encode_units)

# Apply FP-Growth to find frequent itemsets
frequent_itemsets = fpgrowth(basket_sets, min_support=0.05, use_colnames=True)

# Generate association rules
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.0)

# Display frequent itemsets and association rules
print("Frequent Itemsets:")
print(frequent_itemsets)

print("\nAssociation Rules:")
print(rules)

"""Certainly, here is the description of the code steps without plagiarism:

1. The code starts by loading the Online Retail Dataset. You should replace `'OnlineRetail.csv'` with the actual path to your dataset.

2. Data preprocessing steps are carried out, which involve cleaning and preparing the data. This includes tasks like removing spaces in invoice numbers and addressing missing customer IDs to ensure data quality.

3. The data is transformed into a transaction format suitable for the FP-Growth algorithm. This transformation involves creating a one-hot encoded matrix where each row represents an invoice, and items are encoded as binary values (1 for presence, 0 for absence).

4. The FP-Growth algorithm is applied to find frequent itemsets (`frequent_itemsets`). The minimum support threshold is specified, and you can adjust `min_support` as needed to control the sensitivity of the analysis.

5. The code proceeds to generate association rules (`rules`) based on the frequent itemsets. You can specify a minimum lift threshold to control the strength of the associations. In this example, a minimum lift threshold of 1.0 is used, but you can adjust it as needed based on your analysis goals.

6. Finally, the code prints the discovered frequent itemsets and association rules. This step allows for a detailed analysis of the patterns uncovered in the dataset, which can be valuable for understanding customer behavior and making informed business decisions.
"""